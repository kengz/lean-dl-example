# PyTorch LightningModule config
# model architecture using TorchArc
model:
  modules:
    mlp:
      compact:
        layer:
          type: LazyLinear
          keys: [out_features]
          args: [32, 32]
        postlayer:
          - LazyBatchNorm1d:
          - ReLU:
          - Dropout:
              p: 0.1
    classifier:
      LazyLinear:
        out_features: 1

  graph:
    input: x
    modules:
      mlp: [x]
      classifier: [mlp]
    output: classifier

# criterion (loss function)
loss:
  type: BCEWithLogitsLoss

# optimizer
optim:
  type: Adam
  lr: 1e-3

# torchmetrics.MetricCollection for evaluation
metric:
  # e.g. Precision: {num_classes: 3, average: 'macro'}
  Accuracy: { task: binary }
  Precision: { task: binary }
  Recall: { task: binary }
  F1Score: { task: binary }
# which of the metric above to return from main() for Optuna sweep
optuna_metric: F1Score

# Lighting Trainer
trainer:
  precision: 32
  max_epochs: 40 # set to null to auto-stop on convergence
  accelerator: auto # will switch to GPU if available
  log_every_n_steps: 1

onnx:
  path: model.onnx
